{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6d979b-cf07-49a0-ab23-83df1307477c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fac0a07a970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Helpful for computing cosine similarity--Note that this is NOT a similarity!\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Handy command-line argument parsing\n",
    "import argparse\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.corpus import stopwords  # import the stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# We'll use this to save our models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#\n",
    "# IMPORTANT NOTE: Always set your random seeds when dealing with stochastic\n",
    "# algorithms as it lets your bugs be reproducible and (more importantly) it lets\n",
    "# your results be reproducible by others.\n",
    "#\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11071dc5-8d00-4118-9a27-381089ab4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "        self.unique_words = []   # unique word list, also rank by its frequency, but it is not required\n",
    "        self.unique_words_freq = [] # the corresponding frequence list\n",
    "\n",
    "        # A utility data structure that lets us quickly sample \"negative\"\n",
    "        # instances in a context. This table contains unique-ids\n",
    "        self.negative_sampling_table = []\n",
    "        \n",
    "        # The dataset we'll use for training, as a sequence of unqiue word\n",
    "        # ids. This is the sequence across all documents after tokens have been\n",
    "        # randomly subsampled by the word2vec preprocessing step\n",
    "        self.word_sub_prob = {}\n",
    "        self.full_token_sequence_as_ids = []\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        '''\n",
    "        Tokenize the document and returns a list of the tokens\n",
    "        '''\n",
    "        return self.tokenizer.tokenize(text)        \n",
    "\n",
    "    def load_data(self, file_name, min_token_freq):\n",
    "        '''\n",
    "        Reads the data from the specified file as long long sequence of text\n",
    "        (ignoring line breaks) and populates the data structures of this\n",
    "        word2vec object.\n",
    "        '''\n",
    "\n",
    "        # Step 1: Read in the file and create a long sequence of tokens for\n",
    "        # all tokens in the file\n",
    "        data_txt = open(file_name, \"r\").read()\n",
    "        all_tokens = self.tokenizer.tokenize(data_txt)\n",
    "        all_tokens = [i.lower() for i in all_tokens]\n",
    "        print('Reading data and tokenizing')\n",
    "        # return all_tokens\n",
    "    \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        token_counts = Counter(all_tokens)\n",
    "        # return self.word_counts\n",
    "        print('Counting token frequencies')\n",
    "\n",
    "        # Step 3: Replace all tokens below the specified frequency with an <UNK> token. \n",
    "        print(\"Performing minimum thresholding\")\n",
    "        for i in range(len(all_tokens)):\n",
    "            if token_counts[all_tokens[i]] < min_token_freq:\n",
    "                all_tokens[i] = '<UNK>'\n",
    "                \n",
    "        # Step 3.1: Delete all the stopwords\n",
    "        print(\"Performing stopwords removal\")\n",
    "        all_tokens = [i for i in all_tokens if i not in stopwords.words('english')]\n",
    "                \n",
    "        # step4:\n",
    "        self.word_counts = Counter(all_tokens)\n",
    "            \n",
    "        \n",
    "        # Step 5: Create the mapping word and integer\n",
    "        print(\"Performing word mapping\")\n",
    "        word_rank = sorted(self.word_counts.items(), key=lambda x: x[1], reverse=True)   # rank the word_counts based on frequency\n",
    "        word_list = [word[0] for word in word_rank]  # word_rank is a list of tuples.ex[('to', 10)], the word_list is unique list\n",
    "        word_freq = [word[1] for word in word_rank]\n",
    "    \n",
    "        self.unique_words = word_list\n",
    "        self.unique_words_freq = word_freq\n",
    "        \n",
    "        for i in range(len(word_list)):\n",
    "            self.word_to_index[word_list[i]] = i\n",
    "            self.index_to_word[i] = word_list[i]\n",
    "            \n",
    "            \n",
    "        # Step 6: perform word subsampling\n",
    "        print(\"Performing word subsampling\")\n",
    "        word_number = sum(self.word_counts.values())\n",
    "        for w in self.unique_words:\n",
    "            p = self.word_counts[w] / word_number\n",
    "            self.word_sub_prob[w] = (np.sqrt(p/0.001) + 1) * (0.001/p)  # calculate the probability\n",
    "            \n",
    "        # Steo 6.1: keep the word based on the probability\n",
    "        for t in all_tokens:\n",
    "            t_prob = min(1, self.word_sub_prob[t])            \n",
    "            if np.random.choice([0,1], p=[1-t_prob, t_prob]) == 1:\n",
    "                self.full_token_sequence_as_ids.append(self.word_to_index[t])       \n",
    "        \n",
    "        \n",
    "        #self.full_token_sequence_as_ids = [self.word_to_index[word] for word in all_tokens]\n",
    "\n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d tokens (%d unique)' \\\n",
    "              % (file_name, len(self.full_token_sequence_as_ids),\n",
    "                 len(self.word_to_index)))\n",
    "        \n",
    "    def generate_negative_sampling_table(self, exp_power=0.75, table_size=1e6):\n",
    "     \n",
    "        print(\"Generating sampling table\")\n",
    "        # unique words is a sorted word list, now to find the corresponding frequency\n",
    "\n",
    "        power_array = np.power(np.array(self.unique_words_freq), exp_power)\n",
    "        prob_array = power_array / np.sum(power_array)\n",
    "\n",
    "        # print(np.sum(prob_array))  # to check if the summation is 1\n",
    "\n",
    "        # Step 2: Create the table to the correct size. You'll want this to be a\n",
    "        # numpy array of type int\n",
    "        # Step 3: Fill the table so that each word has a number of IDs\n",
    "        # proportionate to its probability of being sampled.\n",
    "        table = np.array([], dtype = np.int32)\n",
    "        for ind, val in enumerate(prob_array):\n",
    "            temp_array = np.full(round(val * table_size), ind)\n",
    "            table = np.hstack((table, temp_array))\n",
    "            \n",
    "        self.negative_sampling_table = table\n",
    "            \n",
    "        return table    \n",
    "        print(len(table))\n",
    "\n",
    "\n",
    "    def generate_negative_samples(self, cur_context_word_id, num_samples):  # the word_id is also a list\n",
    "\n",
    "        results = []\n",
    "        while len(results) < num_samples:\n",
    "            random_neg = np.random.choice(self.negative_sampling_table)  # return an int\n",
    "            if random_neg not in cur_context_word_id:\n",
    "                results.append(random_neg)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f6b237ac-fcdf-4749-a7d3-d0a82ba9d4d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from wiki-bios.med.txt; saw 13713906 tokens (96572 unique)\n",
      "Generating sampling table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0, ..., 96569, 96570, 96571])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus()\n",
    "corpus.load_data('wiki-bios.med.txt', 5)\n",
    "corpus.generate_negative_sampling_table()\n",
    "# corpus.full_token_sequence_as_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "036f4969-6942-44d0-90e3-97dad63d5f2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e8c450322c43b7a97592403931425c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13713906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 2\n",
    "num_negative_samples_per_target = 2\n",
    "\n",
    "training_data = []\n",
    "# looping_times = 10\n",
    "looping_times = len(corpus.full_token_sequence_as_ids)\n",
    "    \n",
    "# Loop through each token in the corpus and generate an instance for each, \n",
    "# adding it to training_data\n",
    "for n in trange(looping_times):\n",
    "    \n",
    "    target = [corpus.full_token_sequence_as_ids[n]]   # convert the target to be a list\n",
    "    instance_tup = ()\n",
    "    \n",
    "    if target != [corpus.word_to_index['<UNK>']]:\n",
    "        if n - window_size < 0:   # left edge of the corpus\n",
    "            context = corpus.full_token_sequence_as_ids[0: n] + corpus.full_token_sequence_as_ids[n+1: n+window_size+1]\n",
    "            negative_sample = corpus.generate_negative_samples(context + target, num_negative_samples_per_target*window_size*2 + 2*window_size - len(context))\n",
    "            word_id = context + negative_sample\n",
    "            predicted_labels = np.hstack((np.full(len(context), 1, dtype=np.float32), np.full(len(negative_sample), 0, dtype=np.float32)))\n",
    "            instance_tup = (np.array(target), np.array(word_id), predicted_labels)\n",
    "\n",
    "        elif n + window_size >= looping_times:   # right edge of the corpus\n",
    "            context = corpus.full_token_sequence_as_ids[n-window_size: n] + corpus.full_token_sequence_as_ids[n+1:looping_times]\n",
    "            negative_sample = corpus.generate_negative_samples(context + target, num_negative_samples_per_target*window_size*2 + 2*window_size - len(context))\n",
    "            word_id = context + negative_sample\n",
    "            predicted_labels = np.hstack((np.full(len(context), 1, dtype=np.float32), np.full(len(negative_sample), 0, dtype=np.float32)))\n",
    "            instance_tup = (np.array(target), np.array(word_id), predicted_labels)\n",
    "\n",
    "        else:\n",
    "            context = corpus.full_token_sequence_as_ids[n-window_size: n] + corpus.full_token_sequence_as_ids[n+1: n+window_size+1]\n",
    "            negative_sample = corpus.generate_negative_samples(context + target, num_negative_samples_per_target*window_size*2)\n",
    "            word_id = context + negative_sample\n",
    "            predicted_labels = np.hstack((np.full(len(context), 1, dtype=np.float32), np.full(len(negative_sample), 0, dtype=np.float32)))\n",
    "            instance_tup = (np.array(target), np.array(word_id), predicted_labels)\n",
    "            \n",
    "        training_data.append(instance_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4bc84c33-1217-4ef3-8112-51666cf0a83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13621971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([56638]),\n",
       " array([  268,  7249,  8092,   407, 28066, 12765,     4, 58690,  2140,\n",
       "         3518, 69058,  1722]),\n",
       " array([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "training_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6e71244-56c2-4b84-8445-06f6bd0027b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.init_emb(init_range=0.5/self.vocab_size)\n",
    "        \n",
    "        self.sm = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def init_emb(self, init_range):\n",
    "        \n",
    "        self.target_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "        \n",
    "    def forward(self, target_word_id, context_word_ids):\n",
    "        \n",
    "        vt = self.target_embeddings(target_word_id)  # target word is a tensor\n",
    "        vc = self.context_embeddings(context_word_ids)\n",
    "        out = torch.bmm(vc, vt.squeeze().unsqueeze(2)).squeeze()\n",
    "        return self.sm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4aa3d78a-3947-483e-b2ef-a2d75694df8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769555f6742d48039acc5fdede9153c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544ee656b3c1443cbfd9ede2ac221b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Set your training stuff, hyperparameters, models, tensorboard writer etc. here\n",
    "batch_size = 64\n",
    "vocab_size = len(corpus.word_to_index)\n",
    "k = 50   # The dimension of the embedding\n",
    "learning_rate = 5e-5\n",
    "epochs = 2\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,    # load the data\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "total_step = len(train_loader)\n",
    "w2v = Word2Vec(vocab_size, k)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(w2v.parameters(), lr=learning_rate)\n",
    "max_steps = 10000\n",
    "\n",
    "writer = SummaryWriter()  # tensorboard summary writer\n",
    "\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss_sum = 0    \n",
    "    # TODO: use your DataLoader to iterate over the data\n",
    "    for step, data in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        # NOTE: since you created the data as a tuple of three np.array instances,\n",
    "        # these have now been converted to Tensor objects for us\n",
    "        target_ids, context_ids, labels = data        \n",
    "        outputs = w2v(target_ids, context_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        loss_sum +=loss.item()\n",
    "        \n",
    "        if (step+1) % 100 == 0:\n",
    "            # print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "            #        .format(epoch+1, epochs, step+1, total_step, loss_sum))\n",
    "            writer.add_scalar(\"Loss/train\", loss_sum, step+1)  # add to the tensor board\n",
    "            loss_sum = 0\n",
    "            \n",
    "        # if step > max_steps:\n",
    "        #     break\n",
    "            \n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2b7e8923-37ee-481d-a2af-88e2662ad3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db36a00b41f442da6a564b406744b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the runtime of different batch sizes\n",
    "\n",
    "batch_size = 512\n",
    "vocab_size = len(corpus.word_to_index)\n",
    "k = 50   # The dimension of the embedding\n",
    "learning_rate = 5e-5\n",
    "epochs = 1\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,    # load the data\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "total_step = len(train_loader)\n",
    "w2v_test = Word2Vec(vocab_size, k)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(w2v_test.parameters(), lr=learning_rate)\n",
    "max_steps = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(tqdm(train_loader)):\n",
    "        target_ids, context_ids, labels = data        \n",
    "        outputs = w2v_test(target_ids, context_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        loss_sum +=loss.item()\n",
    "        if step > max_steps:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dd494c39-4b01-4fe8-b849-d4c1bbacd154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batchsize = [2, 8, 16, 32, 64, 128, 256, 512]\n",
    "expected_runtime = [111, 30, 15, 10, 5, 3, 1.75, 1]\n",
    "\n",
    "x_1 = np.log(np.array(batchsize))\n",
    "y_1 = np.array(expected_runtime)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.plot(x_1, y_1)\n",
    "ax.set_xlabel('batchsize_log'); ax.set_ylabel('expected_runtime')\n",
    "ax.set_title('How batch size affect training time')\n",
    "fig.savefig(\"runtime_batch.jpg\",bbox_inches='tight', dpi=144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "76bd3426-4ba4-40f6-bc5e-1ce85e5ee460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe167e4d1284326982ff58541d9654f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'march', 'score': 0.9984798431396484},\n",
       " {'word': 'october', 'score': 0.9982365965843201},\n",
       " {'word': 'february', 'score': 0.997121274471283},\n",
       " {'word': 'december', 'score': 0.9958194494247437},\n",
       " {'word': 'november', 'score': 0.9956890344619751},\n",
       " {'word': 'september', 'score': 0.9950610399246216},\n",
       " {'word': 'june', 'score': 0.9948214888572693},\n",
       " {'word': 'july', 'score': 0.992501974105835},\n",
       " {'word': 'august', 'score': 0.990580141544342},\n",
       " {'word': 'january', 'score': 0.9894656538963318}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.eval()\n",
    "def get_neighbors(model, word_to_index, target_word):\n",
    "    \"\"\" \n",
    "    Finds the top 10 most similar words to a target word\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for word, index in tqdm(word_to_index.items(), total=len(word_to_index)):\n",
    "        similarity = compute_cosine_similarity(model, word_to_index, target_word, word)\n",
    "        result = {\"word\": word, \"score\": similarity}\n",
    "        outputs.append(result)\n",
    "\n",
    "    # Sort by highest scores\n",
    "    neighbors = sorted(outputs, key=lambda o: o['score'], reverse=True)\n",
    "    return neighbors[1:11]\n",
    "\n",
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    '''\n",
    "    Computes the cosine similarity between the two words\n",
    "    '''\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]))\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]))\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().numpy(),\n",
    "                                      embedding_two.detach().numpy())))\n",
    "    return similarity\n",
    "\n",
    "get_neighbors(w2v, corpus.word_to_index, \"april\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "653dd7df-4194-444a-8fdd-02b9fb27efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, corpus, filename):\n",
    "    '''\n",
    "    Saves the model to the specified filename as a gensim KeyedVectors in the\n",
    "    text format so you can load it separately.\n",
    "    '''\n",
    "\n",
    "    # Creates an empty KeyedVectors with our embedding size\n",
    "    kv = KeyedVectors(vector_size=model.embedding_size)        \n",
    "    vectors = []\n",
    "    words = []\n",
    "    # Get the list of words/vectors in a consistent order\n",
    "    for index in trange(model.target_embeddings.num_embeddings):\n",
    "        word = corpus.index_to_word[index]\n",
    "        vectors.append(model.target_embeddings(torch.LongTensor([index])).detach().numpy()[0])\n",
    "        words.append(word)\n",
    "\n",
    "    # Fills the KV object with our data in the right order\n",
    "    kv.add_vectors(words, vectors) \n",
    "    kv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eb8744dd-582e-4176-9e31-5145b1484f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1257487990bf4bde8d1fe4e72b5bbcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save(w2v, corpus, \"batch64_epoch2.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8eafb-544d-4616-9f2d-46c88a526f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
